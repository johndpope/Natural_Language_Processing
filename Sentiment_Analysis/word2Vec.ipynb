{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 25000 labeled train data, 50000 unlabeled train data and 25000 test data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv(\"/Users/shichangtai/Desktop/Kaggle/Bag of Wprds Meet Bags of Popcorn/labeledTrainData.tsv\",header=0,delimiter='\\t',quoting=3)\n",
    "test=pd.read_csv(\"/Users/shichangtai/Desktop/Kaggle/Bag of Wprds Meet Bags of Popcorn/testData.tsv\",header=0,delimiter='\\t',quoting=3)\n",
    "unlabeled_train=pd.read_csv(\"/Users/shichangtai/Desktop/Kaggle/Bag of Wprds Meet Bags of Popcorn/unlabeledTrainData.tsv\",header=0,delimiter='\\t',quoting=3)\n",
    "print(\"Load %d labeled train data, %d unlabeled train data and %d test data\" % (train[\"review\"].size, unlabeled_train[\"review\"].size, len(test['review'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"8196_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I dont know why people think this is such a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"7166_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"This movie could have been very good, but com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"10633_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I watched this video at a friend's house. I'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"319_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"A friend of mine bought this film for £1, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"8713_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"&lt;br /&gt;&lt;br /&gt;This movie is full of references....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sentiment                                             review\n",
       "0   \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1   \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2   \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3   \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4   \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...\n",
       "5   \"8196_8\"          1  \"I dont know why people think this is such a b...\n",
       "6   \"7166_2\"          0  \"This movie could have been very good, but com...\n",
       "7  \"10633_1\"          0  \"I watched this video at a friend's house. I'm...\n",
       "8    \"319_1\"          0  \"A friend of mine bought this film for £1, and...\n",
       "9  \"8713_10\"          1  \"<br /><br />This movie is full of references...."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Don't remove stop words in Word2Vec\n",
    "# After experiment, we should not use porter stemmer in Word2Vec approach!\n",
    "negator=[]\n",
    "with open(\"/Users/shichangtai/Desktop/Kaggle/Bag of Wprds Meet Bags of Popcorn/negative_words.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        negator.append(line.strip())\n",
    "#print negator\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def review_to_list(raw_review,remove_stopwords=False):\n",
    "    #1. remove html\n",
    "    raw_text=BeautifulSoup(raw_review,'lxml').get_text()\n",
    "    #2. remove non-letter, replace n't (maybe we can hold the number)\n",
    "    only_letter_text=re.sub(\"n\\'t\",\" not\",raw_text)\n",
    "    only_letter_text=re.sub(\"[^a-zA-Z]\",\" \",only_letter_text)\n",
    "    #3. split and lower case\n",
    "    pure_text=only_letter_text.lower().split()\n",
    "    #4. (optional) remove stop words\n",
    "    if remove_stopwords:\n",
    "        #stemmer=nltk.PorterStemmer()\n",
    "        stopword=set(stopwords.words(\"english\"))\n",
    "        pure_text=[m for m in pure_text if m not in stopword or m in set(negator)]\n",
    "    #5. return a list of words\n",
    "    return pure_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Word2Vec input format: single sentences, each one as a list of words. [[list],[list]...]\n",
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def review_to_sentences(review,tokenizer,remove_stopwords=False):\n",
    "    sentences=[]\n",
    "    raw_sentences=tokenizer.tokenize(review.strip())\n",
    "    for item in raw_sentences:\n",
    "        if(len(item)>0):\n",
    "            sentences.append(review_to_list(item,remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:219: UserWarning: \".\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:219: UserWarning: \"..\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "2016-11-03 22:26:40,105:INFO:'pattern' package not found; tag filters are not available for English\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-03 22:26:40,589:INFO:collecting all words and their counts\n",
      "2016-11-03 22:26:40,590:INFO:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2016-11-03 22:26:40,669:INFO:PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2016-11-03 22:26:40,747:INFO:PROGRESS: at sentence #20000, processed 451887 words, keeping 24948 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-03 22:26:40,823:INFO:PROGRESS: at sentence #30000, processed 671310 words, keeping 30034 word types\n",
      "2016-11-03 22:26:40,900:INFO:PROGRESS: at sentence #40000, processed 897810 words, keeping 34348 word types\n",
      "2016-11-03 22:26:40,975:INFO:PROGRESS: at sentence #50000, processed 1116958 words, keeping 37761 word types\n",
      "2016-11-03 22:26:41,046:INFO:PROGRESS: at sentence #60000, processed 1338399 words, keeping 40723 word types\n",
      "2016-11-03 22:26:41,123:INFO:PROGRESS: at sentence #70000, processed 1561573 words, keeping 43333 word types\n",
      "2016-11-03 22:26:41,197:INFO:PROGRESS: at sentence #80000, processed 1780880 words, keeping 45714 word types\n",
      "2016-11-03 22:26:41,274:INFO:PROGRESS: at sentence #90000, processed 2004989 words, keeping 48135 word types\n",
      "2016-11-03 22:26:41,349:INFO:PROGRESS: at sentence #100000, processed 2226945 words, keeping 50207 word types\n",
      "2016-11-03 22:26:41,422:INFO:PROGRESS: at sentence #110000, processed 2446559 words, keeping 52081 word types\n",
      "2016-11-03 22:26:41,495:INFO:PROGRESS: at sentence #120000, processed 2668754 words, keeping 54119 word types\n",
      "2016-11-03 22:26:41,567:INFO:PROGRESS: at sentence #130000, processed 2894274 words, keeping 55847 word types\n",
      "2016-11-03 22:26:41,639:INFO:PROGRESS: at sentence #140000, processed 3106976 words, keeping 57346 word types\n",
      "2016-11-03 22:26:41,715:INFO:PROGRESS: at sentence #150000, processed 3332598 words, keeping 59055 word types\n",
      "2016-11-03 22:26:41,791:INFO:PROGRESS: at sentence #160000, processed 3555286 words, keeping 60617 word types\n",
      "2016-11-03 22:26:41,864:INFO:PROGRESS: at sentence #170000, processed 3778626 words, keeping 62077 word types\n",
      "2016-11-03 22:26:41,953:INFO:PROGRESS: at sentence #180000, processed 3999207 words, keeping 63496 word types\n",
      "2016-11-03 22:26:42,026:INFO:PROGRESS: at sentence #190000, processed 4224420 words, keeping 64794 word types\n",
      "2016-11-03 22:26:42,110:INFO:PROGRESS: at sentence #200000, processed 4448574 words, keeping 66087 word types\n",
      "2016-11-03 22:26:42,218:INFO:PROGRESS: at sentence #210000, processed 4669938 words, keeping 67390 word types\n",
      "2016-11-03 22:26:42,299:INFO:PROGRESS: at sentence #220000, processed 4894939 words, keeping 68697 word types\n",
      "2016-11-03 22:26:42,371:INFO:PROGRESS: at sentence #230000, processed 5117502 words, keeping 69958 word types\n",
      "2016-11-03 22:26:42,448:INFO:PROGRESS: at sentence #240000, processed 5345007 words, keeping 71167 word types\n",
      "2016-11-03 22:26:42,520:INFO:PROGRESS: at sentence #250000, processed 5559122 words, keeping 72351 word types\n",
      "2016-11-03 22:26:42,609:INFO:PROGRESS: at sentence #260000, processed 5779086 words, keeping 73478 word types\n",
      "2016-11-03 22:26:42,683:INFO:PROGRESS: at sentence #270000, processed 6000375 words, keeping 74767 word types\n",
      "2016-11-03 22:26:42,757:INFO:PROGRESS: at sentence #280000, processed 6226254 words, keeping 76369 word types\n",
      "2016-11-03 22:26:42,828:INFO:PROGRESS: at sentence #290000, processed 6449414 words, keeping 77839 word types\n",
      "2016-11-03 22:26:42,904:INFO:PROGRESS: at sentence #300000, processed 6674015 words, keeping 79171 word types\n",
      "2016-11-03 22:26:42,975:INFO:PROGRESS: at sentence #310000, processed 6899329 words, keeping 80480 word types\n",
      "2016-11-03 22:26:43,051:INFO:PROGRESS: at sentence #320000, processed 7124216 words, keeping 81808 word types\n",
      "2016-11-03 22:26:43,126:INFO:PROGRESS: at sentence #330000, processed 7345958 words, keeping 83030 word types\n",
      "2016-11-03 22:26:43,205:INFO:PROGRESS: at sentence #340000, processed 7575470 words, keeping 84280 word types\n",
      "2016-11-03 22:26:43,279:INFO:PROGRESS: at sentence #350000, processed 7798740 words, keeping 85425 word types\n",
      "2016-11-03 22:26:43,354:INFO:PROGRESS: at sentence #360000, processed 8019345 words, keeping 86596 word types\n",
      "2016-11-03 22:26:43,441:INFO:PROGRESS: at sentence #370000, processed 8246537 words, keeping 87708 word types\n",
      "2016-11-03 22:26:43,511:INFO:PROGRESS: at sentence #380000, processed 8471679 words, keeping 88878 word types\n",
      "2016-11-03 22:26:43,595:INFO:PROGRESS: at sentence #390000, processed 8701410 words, keeping 89907 word types\n",
      "2016-11-03 22:26:43,666:INFO:PROGRESS: at sentence #400000, processed 8924359 words, keeping 90916 word types\n",
      "2016-11-03 22:26:43,754:INFO:PROGRESS: at sentence #410000, processed 9145709 words, keeping 91880 word types\n",
      "2016-11-03 22:26:43,830:INFO:PROGRESS: at sentence #420000, processed 9366777 words, keeping 92912 word types\n",
      "2016-11-03 22:26:43,905:INFO:PROGRESS: at sentence #430000, processed 9594314 words, keeping 93932 word types\n",
      "2016-11-03 22:26:43,977:INFO:PROGRESS: at sentence #440000, processed 9821067 words, keeping 94906 word types\n",
      "2016-11-03 22:26:44,046:INFO:PROGRESS: at sentence #450000, processed 10044829 words, keeping 96036 word types\n",
      "2016-11-03 22:26:44,121:INFO:PROGRESS: at sentence #460000, processed 10277589 words, keeping 97088 word types\n",
      "2016-11-03 22:26:44,192:INFO:PROGRESS: at sentence #470000, processed 10505514 words, keeping 97933 word types\n",
      "2016-11-03 22:26:44,270:INFO:PROGRESS: at sentence #480000, processed 10725890 words, keeping 98862 word types\n",
      "2016-11-03 22:26:44,347:INFO:PROGRESS: at sentence #490000, processed 10952634 words, keeping 99871 word types\n",
      "2016-11-03 22:26:44,420:INFO:PROGRESS: at sentence #500000, processed 11174290 words, keeping 100765 word types\n",
      "2016-11-03 22:26:44,496:INFO:PROGRESS: at sentence #510000, processed 11399565 words, keeping 101699 word types\n",
      "2016-11-03 22:26:44,574:INFO:PROGRESS: at sentence #520000, processed 11622901 words, keeping 102598 word types\n",
      "2016-11-03 22:26:44,656:INFO:PROGRESS: at sentence #530000, processed 11847299 words, keeping 103400 word types\n",
      "2016-11-03 22:26:44,746:INFO:PROGRESS: at sentence #540000, processed 12071914 words, keeping 104265 word types\n",
      "2016-11-03 22:26:44,846:INFO:PROGRESS: at sentence #550000, processed 12297452 words, keeping 105133 word types\n",
      "2016-11-03 22:26:44,953:INFO:PROGRESS: at sentence #560000, processed 12518723 words, keeping 105997 word types\n",
      "2016-11-03 22:26:45,067:INFO:PROGRESS: at sentence #570000, processed 12747772 words, keeping 106787 word types\n",
      "2016-11-03 22:26:45,145:INFO:PROGRESS: at sentence #580000, processed 12969256 words, keeping 107664 word types\n",
      "2016-11-03 22:26:45,220:INFO:PROGRESS: at sentence #590000, processed 13194781 words, keeping 108500 word types\n",
      "2016-11-03 22:26:45,290:INFO:PROGRESS: at sentence #600000, processed 13416979 words, keeping 109217 word types\n",
      "2016-11-03 22:26:45,365:INFO:PROGRESS: at sentence #610000, processed 13638002 words, keeping 110091 word types\n",
      "2016-11-03 22:26:45,454:INFO:PROGRESS: at sentence #620000, processed 13864300 words, keeping 110836 word types\n",
      "2016-11-03 22:26:45,568:INFO:PROGRESS: at sentence #630000, processed 14088586 words, keeping 111609 word types\n",
      "2016-11-03 22:26:45,676:INFO:PROGRESS: at sentence #640000, processed 14309369 words, keeping 112415 word types\n",
      "2016-11-03 22:26:45,746:INFO:PROGRESS: at sentence #650000, processed 14535125 words, keeping 113195 word types\n",
      "2016-11-03 22:26:45,816:INFO:PROGRESS: at sentence #660000, processed 14757903 words, keeping 113944 word types\n",
      "2016-11-03 22:26:45,885:INFO:PROGRESS: at sentence #670000, processed 14981287 words, keeping 114642 word types\n",
      "2016-11-03 22:26:45,958:INFO:PROGRESS: at sentence #680000, processed 15206119 words, keeping 115353 word types\n",
      "2016-11-03 22:26:46,040:INFO:PROGRESS: at sentence #690000, processed 15428312 words, keeping 116130 word types\n",
      "2016-11-03 22:26:46,118:INFO:PROGRESS: at sentence #700000, processed 15657015 words, keeping 116943 word types\n",
      "2016-11-03 22:26:46,189:INFO:PROGRESS: at sentence #710000, processed 15879999 words, keeping 117596 word types\n",
      "2016-11-03 22:26:46,285:INFO:PROGRESS: at sentence #720000, processed 16105286 words, keeping 118221 word types\n",
      "2016-11-03 22:26:46,356:INFO:PROGRESS: at sentence #730000, processed 16331667 words, keeping 118954 word types\n",
      "2016-11-03 22:26:46,432:INFO:PROGRESS: at sentence #740000, processed 16552700 words, keeping 119668 word types\n",
      "2016-11-03 22:26:46,504:INFO:PROGRESS: at sentence #750000, processed 16771027 words, keeping 120295 word types\n",
      "2016-11-03 22:26:46,579:INFO:PROGRESS: at sentence #760000, processed 16990419 words, keeping 120930 word types\n",
      "2016-11-03 22:26:46,662:INFO:PROGRESS: at sentence #770000, processed 17217556 words, keeping 121703 word types\n",
      "2016-11-03 22:26:46,736:INFO:PROGRESS: at sentence #780000, processed 17447702 words, keeping 122402 word types\n",
      "2016-11-03 22:26:46,812:INFO:PROGRESS: at sentence #790000, processed 17674778 words, keeping 123066 word types\n",
      "2016-11-03 22:26:46,854:INFO:collected 123504 word types from a corpus of 17797876 raw words and 795538 sentences\n",
      "2016-11-03 22:26:46,855:INFO:Loading a fresh vocabulary\n",
      "2016-11-03 22:26:47,221:INFO:min_count=1 retains 123504 unique words (100% of original 123504, drops 0)\n",
      "2016-11-03 22:26:47,222:INFO:min_count=1 leaves 17797876 word corpus (100% of original 17797876, drops 0)\n",
      "2016-11-03 22:26:47,595:INFO:deleting the raw counts dictionary of 123504 items\n",
      "2016-11-03 22:26:47,602:INFO:sample=0.001 downsamples 48 most-common words\n",
      "2016-11-03 22:26:47,603:INFO:downsampling leaves estimated 13372248 word corpus (75.1% of prior 17797876)\n",
      "2016-11-03 22:26:47,604:INFO:estimated required memory for 123504 words and 100 dimensions: 160555200 bytes\n",
      "2016-11-03 22:26:48,014:INFO:resetting layer weights\n",
      "2016-11-03 22:26:49,710:INFO:training model with 4 workers on 123504 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2016-11-03 22:26:49,710:INFO:expecting 795538 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-11-03 22:26:50,725:INFO:PROGRESS: at 1.02% examples, 679610 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:26:51,737:INFO:PROGRESS: at 2.14% examples, 706590 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:26:52,751:INFO:PROGRESS: at 3.21% examples, 707625 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-03 22:26:53,748:INFO:PROGRESS: at 4.30% examples, 709462 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:26:54,775:INFO:PROGRESS: at 5.38% examples, 708625 words/s, in_qsize 6, out_qsize 0\n",
      "2016-11-03 22:26:55,775:INFO:PROGRESS: at 6.40% examples, 702400 words/s, in_qsize 8, out_qsize 2\n",
      "2016-11-03 22:26:56,788:INFO:PROGRESS: at 7.48% examples, 704331 words/s, in_qsize 7, out_qsize 2\n",
      "2016-11-03 22:26:57,782:INFO:PROGRESS: at 8.55% examples, 705899 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:26:58,783:INFO:PROGRESS: at 9.61% examples, 706469 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:26:59,800:INFO:PROGRESS: at 10.70% examples, 707331 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:00,806:INFO:PROGRESS: at 11.75% examples, 707439 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:01,822:INFO:PROGRESS: at 12.81% examples, 707055 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:27:02,826:INFO:PROGRESS: at 13.88% examples, 707808 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:27:03,844:INFO:PROGRESS: at 14.97% examples, 708949 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-03 22:27:04,861:INFO:PROGRESS: at 16.06% examples, 708843 words/s, in_qsize 8, out_qsize 1\n",
      "2016-11-03 22:27:05,869:INFO:PROGRESS: at 17.08% examples, 706924 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-03 22:27:06,873:INFO:PROGRESS: at 18.18% examples, 708329 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:07,880:INFO:PROGRESS: at 19.25% examples, 708350 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-03 22:27:08,897:INFO:PROGRESS: at 20.32% examples, 708671 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-03 22:27:09,901:INFO:PROGRESS: at 21.33% examples, 706538 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:10,903:INFO:PROGRESS: at 22.36% examples, 705302 words/s, in_qsize 3, out_qsize 1\n",
      "2016-11-03 22:27:11,915:INFO:PROGRESS: at 23.35% examples, 702600 words/s, in_qsize 5, out_qsize 0\n",
      "2016-11-03 22:27:12,915:INFO:PROGRESS: at 24.36% examples, 701337 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:27:13,926:INFO:PROGRESS: at 25.37% examples, 700210 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:27:14,920:INFO:PROGRESS: at 26.42% examples, 699695 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:15,920:INFO:PROGRESS: at 27.48% examples, 700164 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:16,928:INFO:PROGRESS: at 28.55% examples, 700739 words/s, in_qsize 4, out_qsize 0\n",
      "2016-11-03 22:27:17,950:INFO:PROGRESS: at 29.63% examples, 701080 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:18,971:INFO:PROGRESS: at 30.72% examples, 701718 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-03 22:27:19,968:INFO:PROGRESS: at 31.75% examples, 701547 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:20,972:INFO:PROGRESS: at 32.73% examples, 700220 words/s, in_qsize 8, out_qsize 3\n",
      "2016-11-03 22:27:21,966:INFO:PROGRESS: at 33.83% examples, 701285 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:27:22,973:INFO:PROGRESS: at 34.90% examples, 701671 words/s, in_qsize 5, out_qsize 0\n",
      "2016-11-03 22:27:23,984:INFO:PROGRESS: at 35.97% examples, 701778 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:24,984:INFO:PROGRESS: at 37.03% examples, 702080 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:27:26,013:INFO:PROGRESS: at 38.10% examples, 702002 words/s, in_qsize 8, out_qsize 1\n",
      "2016-11-03 22:27:27,021:INFO:PROGRESS: at 39.19% examples, 702352 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:28,022:INFO:PROGRESS: at 40.22% examples, 702230 words/s, in_qsize 7, out_qsize 2\n",
      "2016-11-03 22:27:29,047:INFO:PROGRESS: at 41.31% examples, 702460 words/s, in_qsize 7, out_qsize 4\n",
      "2016-11-03 22:27:30,049:INFO:PROGRESS: at 42.44% examples, 703379 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:31,057:INFO:PROGRESS: at 43.53% examples, 703636 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:32,074:INFO:PROGRESS: at 44.61% examples, 703924 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-03 22:27:33,064:INFO:PROGRESS: at 45.66% examples, 703927 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-03 22:27:34,067:INFO:PROGRESS: at 46.76% examples, 704214 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:35,081:INFO:PROGRESS: at 47.83% examples, 704461 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:36,091:INFO:PROGRESS: at 48.87% examples, 704059 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:27:37,099:INFO:PROGRESS: at 49.94% examples, 704408 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:27:38,104:INFO:PROGRESS: at 51.02% examples, 704653 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-03 22:27:39,106:INFO:PROGRESS: at 52.09% examples, 705075 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:40,115:INFO:PROGRESS: at 53.17% examples, 705270 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:41,115:INFO:PROGRESS: at 54.22% examples, 705258 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:42,133:INFO:PROGRESS: at 55.30% examples, 705456 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:43,134:INFO:PROGRESS: at 56.35% examples, 705443 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-03 22:27:44,133:INFO:PROGRESS: at 57.43% examples, 705702 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:45,133:INFO:PROGRESS: at 58.49% examples, 705712 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:46,144:INFO:PROGRESS: at 59.56% examples, 705722 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:47,146:INFO:PROGRESS: at 60.63% examples, 705928 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:48,153:INFO:PROGRESS: at 61.70% examples, 705963 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:27:49,155:INFO:PROGRESS: at 62.77% examples, 705925 words/s, in_qsize 8, out_qsize 1\n",
      "2016-11-03 22:27:50,164:INFO:PROGRESS: at 63.85% examples, 706063 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:51,188:INFO:PROGRESS: at 64.93% examples, 706005 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:27:52,211:INFO:PROGRESS: at 65.99% examples, 705724 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:53,245:INFO:PROGRESS: at 67.05% examples, 705410 words/s, in_qsize 8, out_qsize 2\n",
      "2016-11-03 22:27:54,249:INFO:PROGRESS: at 68.12% examples, 705390 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-03 22:27:55,256:INFO:PROGRESS: at 69.19% examples, 705597 words/s, in_qsize 6, out_qsize 0\n",
      "2016-11-03 22:27:56,261:INFO:PROGRESS: at 70.22% examples, 705252 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:27:57,276:INFO:PROGRESS: at 71.28% examples, 705317 words/s, in_qsize 6, out_qsize 2\n",
      "2016-11-03 22:27:58,272:INFO:PROGRESS: at 72.32% examples, 705261 words/s, in_qsize 6, out_qsize 0\n",
      "2016-11-03 22:27:59,278:INFO:PROGRESS: at 73.37% examples, 705190 words/s, in_qsize 7, out_qsize 2\n",
      "2016-11-03 22:28:00,286:INFO:PROGRESS: at 74.44% examples, 705329 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-03 22:28:01,293:INFO:PROGRESS: at 75.54% examples, 705692 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:28:02,301:INFO:PROGRESS: at 76.60% examples, 705592 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:28:03,306:INFO:PROGRESS: at 77.66% examples, 705641 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-03 22:28:04,322:INFO:PROGRESS: at 78.71% examples, 705550 words/s, in_qsize 6, out_qsize 2\n",
      "2016-11-03 22:28:05,319:INFO:PROGRESS: at 79.80% examples, 705790 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:28:06,334:INFO:PROGRESS: at 80.89% examples, 705940 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:28:07,341:INFO:PROGRESS: at 81.99% examples, 706148 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:28:08,358:INFO:PROGRESS: at 83.06% examples, 706091 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:28:09,358:INFO:PROGRESS: at 84.15% examples, 706257 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:28:10,366:INFO:PROGRESS: at 85.22% examples, 706256 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:28:11,381:INFO:PROGRESS: at 86.29% examples, 706197 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-03 22:28:12,386:INFO:PROGRESS: at 87.37% examples, 706319 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:28:13,406:INFO:PROGRESS: at 88.44% examples, 706313 words/s, in_qsize 8, out_qsize 1\n",
      "2016-11-03 22:28:14,421:INFO:PROGRESS: at 89.52% examples, 706499 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-03 22:28:15,427:INFO:PROGRESS: at 90.52% examples, 705974 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:28:16,432:INFO:PROGRESS: at 91.54% examples, 705754 words/s, in_qsize 6, out_qsize 0\n",
      "2016-11-03 22:28:17,432:INFO:PROGRESS: at 92.54% examples, 705358 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:28:18,437:INFO:PROGRESS: at 93.58% examples, 705235 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:28:19,451:INFO:PROGRESS: at 94.64% examples, 705129 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:28:20,462:INFO:PROGRESS: at 95.63% examples, 704639 words/s, in_qsize 6, out_qsize 2\n",
      "2016-11-03 22:28:21,463:INFO:PROGRESS: at 96.71% examples, 704787 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-03 22:28:22,474:INFO:PROGRESS: at 97.76% examples, 704779 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-03 22:28:23,482:INFO:PROGRESS: at 98.83% examples, 704722 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:28:24,493:INFO:PROGRESS: at 99.89% examples, 704798 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-03 22:28:24,571:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2016-11-03 22:28:24,578:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2016-11-03 22:28:24,580:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2016-11-03 22:28:24,589:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2016-11-03 22:28:24,589:INFO:training on 88989380 raw words (66863480 effective words) took 94.9s, 704794 effective words/s\n",
      "2016-11-03 22:28:24,590:INFO:precomputing L2-norms of word weight vectors\n",
      "2016-11-03 22:28:25,575:INFO:saving Word2Vec object under word2vec_5000features_1minwords_10context, separately None\n",
      "2016-11-03 22:28:25,576:INFO:storing numpy array 'syn1neg' to word2vec_5000features_1minwords_10context.syn1neg.npy\n",
      "2016-11-03 22:28:25,702:INFO:not storing attribute syn0norm\n",
      "2016-11-03 22:28:25,702:INFO:storing numpy array 'syn0' to word2vec_5000features_1minwords_10context.syn0.npy\n",
      "2016-11-03 22:28:25,871:INFO:not storing attribute cum_table\n",
      "2016-11-03 22:28:27,986:INFO:saved word2vec_5000features_1minwords_10context\n"
     ]
    }
   ],
   "source": [
    "#word2Vec method\n",
    "sentences=[]\n",
    "print(\"Parsing sentences from training data...\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences+=review_to_sentences(review.decode('utf-8'),tokenizer)\n",
    "print(\"Parsing sentences from unlabeled training data...\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences+=review_to_sentences(review.decode('utf-8'),tokenizer)\n",
    "print len(sentences)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s',level=logging.INFO)\n",
    "\n",
    "num_features=100\n",
    "min_word_count=1\n",
    "num_workers=4\n",
    "context=10\n",
    "downsampling=1e-3\n",
    "\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model=word2vec.Word2Vec(sentences,workers=num_workers,\\\n",
    "                        size=num_features,min_count=min_word_count,\\\n",
    "                        window=context,sample=downsampling)\n",
    "model.init_sims(replace=True)\n",
    "model_name=\"word2vec_100features_1minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model=Word2Vec.load(\"word2vec_5000features_40minwords_10context\")\n",
    "num_features=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'man'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man child woman mother\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ape', 0.6768560409545898),\n",
       " (u'spider', 0.6629838347434998),\n",
       " (u'eagle', 0.6501494646072388),\n",
       " (u'octopus', 0.6344279646873474),\n",
       " (u'cobra', 0.6295153498649597),\n",
       " (u'elephant', 0.6254611015319824),\n",
       " (u'tiger', 0.6231738328933716),\n",
       " (u'blade', 0.5876468420028687),\n",
       " (u'sword', 0.5871504545211792),\n",
       " (u'airplane', 0.5838525891304016)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"iron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490, 5000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "#model=Word2Vec.load(\"300features_40minwords_10context\")\n",
    "model.syn0.shape\n",
    "#model[\"iron\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Creating review features vec for test data\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "#word2Vec method\n",
    "import numpy as np\n",
    "def makeFeatureVec(words,model,numFeatures):\n",
    "    res=np.zeros((numFeatures,),dtype='float32')\n",
    "    count=0\n",
    "    index2word_set=set(model.index2word)\n",
    "    for item in words:\n",
    "        if item in index2word_set:\n",
    "            count+=1;\n",
    "            res=np.add(res,model[item])\n",
    "    res=np.divide(res,count)\n",
    "    return res\n",
    "def getAvgFeatureVecs(review,model,numFeatures):\n",
    "    reviewFeatureVecs=np.zeros((len(review),numFeatures))\n",
    "    count=0\n",
    "    for item in review:\n",
    "        if count%1000.==0.:\n",
    "            print(\"Review %d of %d\" % (count,len(review)))\n",
    "        reviewFeatureVecs[count]=makeFeatureVec(item,model,numFeatures)\n",
    "        count+=1\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "clean_train_reviews=[]\n",
    "for m in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_list(m,remove_stopwords=True))\n",
    "train_data_average=getAvgFeatureVecs(clean_train_reviews,model,num_features)\n",
    "print(\"Creating review features vec for test data\")\n",
    "clean_test_reviews=[]\n",
    "for m in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_list(m,remove_stopwords=True))\n",
    "test_data_average=getAvgFeatureVecs(clean_test_reviews,model,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled train data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "print(\"Fitting a random forest to labeled train data...\")\n",
    "forest=clf.fit(train_data_average,train[\"sentiment\"])\n",
    "result=forest.predict(test_data_average)\n",
    "output=pd.DataFrame(data={\"id\":test['id'],\"sentiment\":result})\n",
    "output.to_csv(\"/Users/shichangtai/Desktop/Kaggle/Bag of Wprds Meet Bags of Popcorn/result_Word2Vec.csv\",index=False,quoting=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression(class_weight=\"balanced\")\n",
    "logistic=clf.fit(train_data_average,train[\"sentiment\"])\n",
    "result=logistic.predict_proba(test_data_average)[:,1]\n",
    "output=pd.DataFrame(data={\"id\":test['id'],\"sentiment\":result})\n",
    "output.to_csv(\"/Users/shichangtai/Desktop/Kaggle/Bag of Wprds Meet Bags of Popcorn/result_Word2Vec.csv\",index=False,quoting=3)\n",
    "#score of , word2vec, C default\n",
    "#score of 0.928, word2vec, balanced class weight 5000features_40minwords_10context\n",
    "#score of , word2vec, balanced class weight 100features_1minwords_10context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans algorithm take time:  1645.09027004  seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "start=time.time()\n",
    "wordVec=model.syn0\n",
    "num_clusters=wordVec.shape[0]/5\n",
    "kmeans_clustering=KMeans(n_clusters=num_clusters)\n",
    "idx=kmeans_clustering.fit_predict(wordVec)\n",
    "end=time.time()\n",
    "elapse=end-start\n",
    "print \"Kmeans algorithm take time: \",elapse,\" seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordCentroidMap=dict(zip(model.index2word,idx))\n",
    "def create_bag_of_centroids(wordList,wordCentroidMap):\n",
    "    num_centroids=max(wordCentroidMap.values())+1\n",
    "    bag_of_centroids=np.zeros(num_centroids,dtype=\"float32\")\n",
    "    for word in wordList:\n",
    "        if word in wordCentroidMap:\n",
    "            bag_of_centroids[wordCentroidMap[word]]+=1\n",
    "    return bag_of_centroids    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_centroid=np.zeros((train[\"review\"].size,num_clusters),dtype=\"float32\")\n",
    "counter=0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroid[counter]=create_bag_of_centroids(review,wordCentroidMap)\n",
    "    counter+=1\n",
    "\n",
    "test_centroid=np.zeros((test[\"review\"].size,num_clusters),dtype=\"float32\")\n",
    "counter=0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroid[counter]=create_bag_of_centroids(review,wordCentroidMap)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest=forest.fit(train_centroid,train[\"sentiment\"])\n",
    "result2=forest.predict(test_centroid)\n",
    "output=pd.DataFrame(data={\"id\":test[\"id\"],\"sentiment\":result2})\n",
    "output.to_csv(\"/Users/shichangtai/Desktop/Kaggle/Bag of Wprds Meet Bags of Popcorn/result3.csv\",index=False,quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
