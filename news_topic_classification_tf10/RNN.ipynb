{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.preprocessing import text\n",
    "import numpy as np\n",
    "import data_dealer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter embedding_size: 200\n",
      "parameter epoch_size: 10\n",
      "parameter batch_size: 300\n",
      "parameter dropout_keep_probability: 0.7\n",
      "parameter evaluate_step: 50\n",
      "parameter train_dev_split_ratio: 0.98\n",
      "parameter topic_num: 3\n",
      "parameter hidden_size: 64\n",
      "parameter shuffle_input: True\n"
     ]
    }
   ],
   "source": [
    "tf.flags.DEFINE_integer(\"epoch_size\",10,\"Default 10\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\",300,\"Default 300\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_step\",50,\"Evaluate each 50[default] global steps\")\n",
    "tf.flags.DEFINE_integer(\"embedding_size\",200,\"Default 200\")\n",
    "tf.flags.DEFINE_integer(\"hidden_size\",64,\"Default 64\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_probability\",0.9,\"Probability of keep neuron, default 0.9\")\n",
    "tf.flags.DEFINE_integer(\"topic_num\",3,\"The number of different news topics, it depends on news corpus.\")\n",
    "tf.flags.DEFINE_bool(\"shuffle_input\",True,\"Default True\")\n",
    "tf.flags.DEFINE_float(\"train_dev_split_ratio\",0.98,\"Default 0.01, 98% is training data, 2% is development data\")\n",
    "\n",
    "FLAGS=tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "\n",
    "for para,val in FLAGS.__flags.items():\n",
    "    print(\"parameter %s: %s\"%(para,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class topicRNN(object):\n",
    "    def __init__(self, vocabulary_size, embedding_size, hidden_size, max_news_size, topic_size):\n",
    "        #input information\n",
    "        self.input_news=tf.placeholder(tf.int64, [None, max_news_size], name='input_news')\n",
    "        self.input_topic=tf.placeholder(tf.int64, [None], name='input_topic')\n",
    "        self.early_stop=tf.placeholder(tf.int32, [None],name='early_stop')\n",
    "        self.dropout_keep_probability=tf.placeholder(tf.float32, name='dropout_keep_probability')\n",
    "        #embedding size, notice that GPU cannot used in embedding.\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding'):\n",
    "            embedding_matrix=tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1,1), name='embed_matrix')\n",
    "            #self.embedding_news : [None(batch_size), max_news_size, embedding_size]\n",
    "            self.embedding_news=tf.nn.embedding_lookup(embedding_matrix,self.input_news)\n",
    "        self.X= tf.transpose(self.embedding_news,[1,0,2])\n",
    "        self.X= tf.reshape(self.X,[-1,embedding_size])\n",
    "        #Splits a tensor into num_split tensors along one dimension\n",
    "        #a list of 'time_steps' tensors of shape (batch_size, embedding_size)\n",
    "        self.X= tf.split(0,max_news_size,self.X)\n",
    "        lstm_cell=tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "        outputs,states=tf.nn.rnn(lstm_cell,self.X,dtype=tf.float32,sequence_length=self.early_stop)\n",
    "        # 'outputs' is a list of output at every timestep\n",
    "        # pack them in a Tensor\n",
    "        outputs = tf.pack(outputs)\n",
    "        #[None(batch_size), max_news_size, hidden_size]\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        #only the last output in one news is important\n",
    "        batch_size=tf.shape(outputs)[0]\n",
    "        index=tf.range(0, batch_size)*max_news_size+self.early_stop\n",
    "        #[None(batch_size), hidden_size]\n",
    "        outputs=tf.gather(tf.reshape(outputs,[-1,hidden_size]),index)\n",
    "        \n",
    "        with tf.name_scope('drop_out'):\n",
    "            outputs=tf.nn.dropout(outputs,self.dropout_keep_probability)\n",
    "        \n",
    "        \n",
    "        #unnormalized scores and prediction\n",
    "        with tf.name_scope('output'):\n",
    "            w=tf.Variable(tf.truncated_normal([hidden_size,topic_size],stddev=0.1),name='w')\n",
    "            b=tf.Variable(tf.constant(0.1,shape=[topic_size]),name='b')\n",
    "            self.scores=tf.nn.xw_plus_b(outputs,w,b,name='scores')\n",
    "            self.prediction=tf.argmax(self.scores,1,name='prediction')\n",
    "        \n",
    "        #loss\n",
    "        with tf.name_scope('loss'):\n",
    "            losses=tf.nn.sparse_softmax_cross_entropy_with_logits(self.scores, self.input_topic)\n",
    "            self.loss=tf.reduce_mean(losses,name='loss')\n",
    "            \n",
    "        #accuracy\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_pre=tf.equal(self.prediction,self.input_topic)\n",
    "            self.accuracy=tf.reduce_mean(tf.cast(correct_pre,'float'),name='accuracy')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading news and topic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.211 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has beend imported!\n",
      "Sub-data has imported 0.0 percentage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-data has imported 9.99995750696 percentage\n",
      "Sub-data has imported 19.9999150139 percentage\n",
      "Sub-data has imported 29.9998725209 percentage\n",
      "Sub-data has imported 39.9998300278 percentage\n",
      "Sub-data has imported 49.9997875348 percentage\n",
      "Sub-data has imported 59.9997450418 percentage\n",
      "Sub-data has imported 69.9997025487 percentage\n",
      "Sub-data has imported 79.9996600557 percentage\n",
      "Sub-data has imported 89.9996175627 percentage\n",
      "Sub-data has imported 99.9995750696 percentage\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading news and topic...\")\n",
    "all_urls, all_titles, all_news=data_dealer.import_data()\n",
    "#data is a dictionary, comprised of health, auto, business, it, sports, learning, news, yule 10001 respectively.\n",
    "data=data_dealer.subData(all_urls, all_titles, all_news)\n",
    "health=zip(data['health'],np.ones([10001,1]))\n",
    "auto=zip(data['auto'],2*np.ones([10001,1]))\n",
    "business=zip(data['business'],3*np.ones([10001,1]))\n",
    "x_news=data['health']+data['auto']+data['business']\n",
    "y_label=[0]*10001+[1]*10001+[2]*10001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal length in all news: 1026\n",
      "There are 134044 Chinese vocabulary in all the news corpus.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "early_stop_index=np.array([len(x.split(\" \"))-1 for x in x_news])\n",
    "max_news_length=max(early_stop_index)\n",
    "words_to_num=learn.preprocessing.VocabularyProcessor(max_news_length)\n",
    "print('Maximal length in all news: %s' % max_news_length)\n",
    "x_nums=np.array(list(words_to_num.fit_transform(x_news)))\n",
    "vocabulary_size=len(words_to_num.vocabulary_)\n",
    "print(\"There are %s Chinese vocabulary in all the news corpus.\" % vocabulary_size)\n",
    "#processor.reverse(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle input data...\n",
      "Split input data into training and development part...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/m/home/home8/80/shic1/unix/.local/lib/python2.7/site-packages/ipykernel/__main__.py:10: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/m/home/home8/80/shic1/unix/.local/lib/python2.7/site-packages/ipykernel/__main__.py:11: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/m/home/home8/80/shic1/unix/.local/lib/python2.7/site-packages/ipykernel/__main__.py:12: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/m/home/home8/80/shic1/unix/.local/lib/python2.7/site-packages/ipykernel/__main__.py:13: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/m/home/home8/80/shic1/unix/.local/lib/python2.7/site-packages/ipykernel/__main__.py:14: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/m/home/home8/80/shic1/unix/.local/lib/python2.7/site-packages/ipykernel/__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.shuffle_input:\n",
    "    print(\"Shuffle input data...\")\n",
    "    np.random.seed(1)\n",
    "    new_indices=np.random.permutation(range(len(y_label)))\n",
    "    x_nums=x_nums[new_indices]\n",
    "    y_label=np.array(y_label)[new_indices]\n",
    "    early_stop_index=early_stop_index[new_indices]\n",
    "\n",
    "print(\"Split input data into training and development part...\")\n",
    "x_train=x_nums[:FLAGS.train_dev_split_ratio*len(y_label),:]\n",
    "y_train=y_label[:FLAGS.train_dev_split_ratio*len(y_label)]\n",
    "early_stop_index_train=early_stop_index[:FLAGS.train_dev_split_ratio*len(y_label)]\n",
    "x_dev=x_nums[FLAGS.train_dev_split_ratio*len(y_label):,:]\n",
    "y_dev=y_label[FLAGS.train_dev_split_ratio*len(y_label):]\n",
    "early_stop_index_dev=early_stop_index[FLAGS.train_dev_split_ratio*len(y_label):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Start training RNN model...--------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------Start training RNN model...--------------------\")\n",
    "gra=tf.Graph()\n",
    "with gra.as_default():\n",
    "    sess=tf.Session()\n",
    "    with sess.as_default():   \n",
    "        rnn=topicRNN(vocabulary_size=vocabulary_size,embedding_size=FLAGS.embedding_size,\n",
    "                         hidden_size=FLAGS.hidden_size,\n",
    "                         max_news_size=max_news_length,topic_size=3)\n",
    "        print('RNN Model has been built!')\n",
    "        global_step=tf.Variable(0,name=\"global_step\",trainable=False)\n",
    "        optimizer=tf.train.AdamOptimizer()\n",
    "        gradient_and_variable=optimizer.compute_gradients(rnn.loss)\n",
    "        train_op=optimizer.apply_gradients(gradient_and_variable,global_step=global_step)\n",
    "        \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        def train_one_step(x_batch,y_batch,early_stop_batch):\n",
    "            feed_dict={rnn.input_news:x_batch,rnn.input_topic:y_batch,\n",
    "                       rnn.dropout_keep_probability:FLAGS.dropout_keep_probability,\n",
    "                       rnn.early_stop:early_stop_batch}\n",
    "            _,step,loss,accuracy=sess.run([train_op,global_step,rnn.loss,rnn.accuracy],feed_dict)\n",
    "            print(\"Train processing: step {}, loss {}, accuracy {}\".format(step,loss,accuracy))\n",
    "        \n",
    "        def dev_one_step(x_batch,y_batch,early_stop_batch):\n",
    "            feed_dict={rnn.input_news:x_batch,rnn.input_topic:y_batch,\n",
    "                       rnn.dropout_keep_probability:1.0,rnn.early_stop:early_stop_batch}\n",
    "            step,loss,accuracy=sess.run([global_step,rnn.loss,rnn.accuracy],feed_dict)\n",
    "            print(\"Dev processing: step {}, loss {}, accuracy {}\".format(step,loss,accuracy))\n",
    "        \n",
    "        \n",
    "        for epo in range(FLAGS.epoch_size):\n",
    "            print('---------------Epoch: %s---------------' % epo)\n",
    "            # input data in each epoch is not be permutated!\n",
    "            for i in range(len(y_train)//FLAGS.batch_size):\n",
    "                x_temp=x_train[i*FLAGS.batch_size:(i+1)*FLAGS.batch_size]\n",
    "                y_temp=y_train[i*FLAGS.batch_size:(i+1)*FLAGS.batch_size]\n",
    "                early_stop_temp=early_stop_index_train[i*FLAGS.batch_size:(i+1)*FLAGS.batch_size]\n",
    "                train_one_step(x_temp,y_temp,early_stop_temp)\n",
    "                current_step=tf.train.global_step(sess,global_step)\n",
    "                if current_step % FLAGS.evaluate_step==0:\n",
    "                    print(\"Evalution start... at step %s\"%current_step)\n",
    "                    dev_one_step(x_dev,y_dev,early_stop_index_dev)\n",
    "                    print(\"Evaluation end\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
