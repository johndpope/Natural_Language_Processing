{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.preprocessing import text\n",
    "import numpy as np\n",
    "import cnn\n",
    "import data_dealer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter embedding_size: 200\n",
      "parameter keep_prob: 0.7\n",
      "parameter batch_size: 500\n",
      "parameter evaluate_step: 100\n",
      "parameter filter_size: 2,3,4\n",
      "parameter train_dev_split_ratio: 0.99\n",
      "parameter topic_num: 12\n",
      "parameter epoch_size: 10\n",
      "parameter filter_num: 3\n",
      "parameter shuffle_input: False\n"
     ]
    }
   ],
   "source": [
    "tf.flags.DEFINE_integer(\"epoch_size\",10,\"Default 10\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\",300,\"Default 300\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_step\",50,\"Evaluate each 50[default] global steps\")\n",
    "tf.flags.DEFINE_integer(\"embedding_size\",200,\"Default 200\")\n",
    "tf.flags.DEFINE_string(\"filter_size\",'2,3,4',\"Default '2,3,4' \")\n",
    "tf.flags.DEFINE_integer(\"filter_num\",3,\"Filter numbers for each kind of filter, default 3\")\n",
    "tf.flags.DEFINE_float(\"keep_prob\",0.7,\"Probability of keep neuron when dropout, default 0.7\")\n",
    "tf.flags.DEFINE_integer(\"topic_num\",12,\"The number of different news topics, it depends on news corpus.\")\n",
    "tf.flags.DEFINE_bool(\"shuffle_input\",False,\"Default False\")\n",
    "tf.flags.DEFINE_float(\"train_dev_split_ratio\",0.98,\"Default 0.01, 98% is training data, 2% is development data\")\n",
    "\n",
    "FLAGS=tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "\n",
    "for para,val in FLAGS.__flags.items():\n",
    "    print(\"parameter %s: %s\"%(para,val))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading news and topic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.205 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has beend imported!\n",
      "Sub-data has imported 0.0 percentage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-data has imported 9.99995750696 percentage\n",
      "Sub-data has imported 19.9999150139 percentage\n",
      "Sub-data has imported 29.9998725209 percentage\n",
      "Sub-data has imported 39.9998300278 percentage\n",
      "Sub-data has imported 49.9997875348 percentage\n",
      "Sub-data has imported 59.9997450418 percentage\n",
      "Sub-data has imported 69.9997025487 percentage\n",
      "Sub-data has imported 79.9996600557 percentage\n",
      "Sub-data has imported 89.9996175627 percentage\n",
      "Sub-data has imported 99.9995750696 percentage\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading news and topic...\")\n",
    "all_urls, all_titles, all_news=data_dealer.import_data()\n",
    "#data is a dictionary, comprised of health, auto, business, it, sports, learning, news, yule 10001 respectively.\n",
    "data=data_dealer.subData(all_urls, all_titles, all_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "health=zip(data['health'],np.ones([10001,1]))\n",
    "auto=zip(data['auto'],2*np.ones([10001,1]))\n",
    "business=zip(data['business'],3*np.ones([10001,1]))\n",
    "x_news=data['health']+data['auto']+data['business']\n",
    "y_label=[0]*10001+[1]*10001+[2]*10001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal length in all news: 1027\n",
      "There are 134044 Chinese vocabulary in all the news corpus.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "max_news_length=max([len(x.split(\" \")) for x in x_news])\n",
    "words_to_num=learn.preprocessing.VocabularyProcessor(max_news_length)\n",
    "print('Maximal length in all news: %s' % max_news_length)\n",
    "x_nums=np.array(list(words_to_num.fit_transform(x_news)))\n",
    "vocabulary_size=len(words_to_num.vocabulary_)\n",
    "print(\"There are %s Chinese vocabulary in all the news corpus.\" % vocabulary_size)\n",
    "#processor.reverse(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FLAGS.shuffle_input=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle input data...\n",
      "Split input data into training and development part...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/m/home/home8/80/shic1/unix/.local/lib/python2.7/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/m/home/home8/80/shic1/unix/.local/lib/python2.7/site-packages/ipykernel/__main__.py:10: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/m/home/home8/80/shic1/unix/.local/lib/python2.7/site-packages/ipykernel/__main__.py:11: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/m/home/home8/80/shic1/unix/.local/lib/python2.7/site-packages/ipykernel/__main__.py:12: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.shuffle_input:\n",
    "    print(\"Shuffle input data...\")\n",
    "    np.random.seed(1)\n",
    "    new_indices=np.random.permutation(range(len(y_label)))\n",
    "    x_nums=x_nums[new_indices]\n",
    "    y_label=np.array(y_label)[new_indices]\n",
    "\n",
    "print(\"Split input data into training and development part...\")\n",
    "x_train=x_nums[:FLAGS.train_dev_split_ratio*len(y_label),:]\n",
    "y_train=y_label[:FLAGS.train_dev_split_ratio*len(y_label)]\n",
    "x_dev=x_nums[FLAGS.train_dev_split_ratio*len(y_label):,:]\n",
    "y_dev=y_label[FLAGS.train_dev_split_ratio*len(y_label):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29702.97"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1]\n",
    "len(x_train[1*FLAGS.batch_size:(1+1)*FLAGS.batch_size])\n",
    "len(x_dev)\n",
    "FLAGS.train_dev_split_ratio*len(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Start training model...--------------------\n",
      "Train processing: step 1, loss 1.19139623642, accuracy 0.375\n",
      "Train processing: step 2, loss 1.26080417633, accuracy 0.265625\n",
      "Train processing: step 3, loss 1.1573445797, accuracy 0.390625\n",
      "Train processing: step 4, loss 1.15334033966, accuracy 0.421875\n",
      "Train processing: step 5, loss 1.21113753319, accuracy 0.328125\n",
      "Train processing: step 6, loss 1.06820511818, accuracy 0.484375\n",
      "Train processing: step 7, loss 1.13968324661, accuracy 0.421875\n",
      "Train processing: step 8, loss 1.16829836369, accuracy 0.390625\n",
      "Train processing: step 9, loss 1.13090252876, accuracy 0.40625\n",
      "Train processing: step 10, loss 1.08605706692, accuracy 0.40625\n",
      "Train processing: step 11, loss 1.14099144936, accuracy 0.40625\n",
      "Train processing: step 12, loss 1.1098177433, accuracy 0.4375\n",
      "Train processing: step 13, loss 1.13232040405, accuracy 0.390625\n",
      "Train processing: step 14, loss 1.09934997559, accuracy 0.421875\n",
      "Train processing: step 15, loss 1.12204360962, accuracy 0.484375\n",
      "Train processing: step 16, loss 1.0036046505, accuracy 0.515625\n",
      "Train processing: step 17, loss 1.07246947289, accuracy 0.421875\n",
      "Train processing: step 18, loss 1.16019451618, accuracy 0.40625\n",
      "Train processing: step 19, loss 1.06270074844, accuracy 0.5\n",
      "Train processing: step 20, loss 0.941400647163, accuracy 0.578125\n",
      "Train processing: step 21, loss 1.07814526558, accuracy 0.484375\n",
      "Train processing: step 22, loss 1.05680084229, accuracy 0.53125\n",
      "Train processing: step 23, loss 0.921292185783, accuracy 0.609375\n",
      "Train processing: step 24, loss 0.976105868816, accuracy 0.546875\n",
      "Train processing: step 25, loss 1.01122820377, accuracy 0.5\n",
      "Train processing: step 26, loss 1.00364935398, accuracy 0.53125\n",
      "Train processing: step 27, loss 1.01918947697, accuracy 0.46875\n",
      "Train processing: step 28, loss 0.95895409584, accuracy 0.5\n",
      "Train processing: step 29, loss 0.973723769188, accuracy 0.453125\n",
      "Train processing: step 30, loss 0.955858111382, accuracy 0.59375\n",
      "Train processing: step 31, loss 1.04156851768, accuracy 0.484375\n",
      "Train processing: step 32, loss 0.899986565113, accuracy 0.546875\n",
      "Train processing: step 33, loss 1.03384208679, accuracy 0.5\n",
      "Train processing: step 34, loss 0.936045348644, accuracy 0.546875\n",
      "Train processing: step 35, loss 0.911004662514, accuracy 0.546875\n",
      "Train processing: step 36, loss 1.02484679222, accuracy 0.5625\n",
      "Train processing: step 37, loss 1.04675960541, accuracy 0.390625\n",
      "Train processing: step 38, loss 0.949945271015, accuracy 0.546875\n",
      "Train processing: step 39, loss 0.913749217987, accuracy 0.578125\n",
      "Train processing: step 40, loss 0.944467246532, accuracy 0.609375\n",
      "Train processing: step 41, loss 0.899327397346, accuracy 0.65625\n",
      "Train processing: step 42, loss 0.944599449635, accuracy 0.5625\n",
      "Train processing: step 43, loss 0.963433563709, accuracy 0.515625\n",
      "Train processing: step 44, loss 0.915484368801, accuracy 0.515625\n",
      "Train processing: step 45, loss 1.07541143894, accuracy 0.421875\n",
      "Train processing: step 46, loss 0.913889527321, accuracy 0.53125\n",
      "Train processing: step 47, loss 0.909844636917, accuracy 0.59375\n",
      "Train processing: step 48, loss 0.9159116745, accuracy 0.5625\n",
      "Train processing: step 49, loss 0.95932674408, accuracy 0.484375\n",
      "Train processing: step 50, loss 1.06825196743, accuracy 0.484375\n",
      "Train processing: step 51, loss 0.857872366905, accuracy 0.625\n",
      "Train processing: step 52, loss 1.01337087154, accuracy 0.546875\n",
      "Train processing: step 53, loss 0.972209334373, accuracy 0.546875\n",
      "Train processing: step 54, loss 0.950412273407, accuracy 0.53125\n",
      "Train processing: step 55, loss 0.97319149971, accuracy 0.5625\n",
      "Train processing: step 56, loss 0.981520712376, accuracy 0.5625\n",
      "Train processing: step 57, loss 0.837768733501, accuracy 0.65625\n",
      "Train processing: step 58, loss 0.736659705639, accuracy 0.734375\n",
      "Train processing: step 59, loss 0.893546402454, accuracy 0.625\n",
      "Train processing: step 60, loss 0.871301233768, accuracy 0.578125\n",
      "Train processing: step 61, loss 0.85957545042, accuracy 0.609375\n",
      "Train processing: step 62, loss 0.886796712875, accuracy 0.625\n",
      "Train processing: step 63, loss 0.803371965885, accuracy 0.65625\n",
      "Train processing: step 64, loss 0.7216796875, accuracy 0.703125\n",
      "Train processing: step 65, loss 0.813387453556, accuracy 0.671875\n",
      "Train processing: step 66, loss 0.792303681374, accuracy 0.703125\n",
      "Train processing: step 67, loss 0.804383099079, accuracy 0.671875\n",
      "Train processing: step 68, loss 0.926980137825, accuracy 0.546875\n",
      "Train processing: step 69, loss 0.765076160431, accuracy 0.6875\n",
      "Train processing: step 70, loss 0.7555757761, accuracy 0.6875\n",
      "Train processing: step 71, loss 0.922415614128, accuracy 0.59375\n",
      "Train processing: step 72, loss 0.704805135727, accuracy 0.65625\n",
      "Train processing: step 73, loss 0.809857010841, accuracy 0.625\n",
      "Train processing: step 74, loss 0.721155047417, accuracy 0.671875\n",
      "Train processing: step 75, loss 0.854037821293, accuracy 0.640625\n",
      "Train processing: step 76, loss 0.804575800896, accuracy 0.640625\n",
      "Train processing: step 77, loss 0.848106861115, accuracy 0.609375\n",
      "Train processing: step 78, loss 0.87302172184, accuracy 0.59375\n",
      "Train processing: step 79, loss 0.813148975372, accuracy 0.609375\n",
      "Train processing: step 80, loss 0.778350114822, accuracy 0.65625\n",
      "Train processing: step 81, loss 0.796130180359, accuracy 0.640625\n",
      "Train processing: step 82, loss 0.775862216949, accuracy 0.65625\n",
      "Train processing: step 83, loss 0.686909556389, accuracy 0.71875\n",
      "Train processing: step 84, loss 0.799614548683, accuracy 0.6875\n",
      "Train processing: step 85, loss 0.644120335579, accuracy 0.765625\n",
      "Train processing: step 86, loss 0.71961826086, accuracy 0.671875\n",
      "Train processing: step 87, loss 0.872413754463, accuracy 0.703125\n",
      "Train processing: step 88, loss 0.827879309654, accuracy 0.625\n",
      "Train processing: step 89, loss 0.797807812691, accuracy 0.703125\n",
      "Train processing: step 90, loss 0.725096821785, accuracy 0.6875\n",
      "Train processing: step 91, loss 0.839480519295, accuracy 0.65625\n",
      "Train processing: step 92, loss 0.851055979729, accuracy 0.625\n",
      "Train processing: step 93, loss 0.776183784008, accuracy 0.640625\n",
      "Train processing: step 94, loss 0.705538213253, accuracy 0.75\n",
      "Train processing: step 95, loss 0.868183732033, accuracy 0.625\n",
      "Train processing: step 96, loss 0.906293272972, accuracy 0.5625\n",
      "Train processing: step 97, loss 0.643469691277, accuracy 0.734375\n",
      "Train processing: step 98, loss 0.690148830414, accuracy 0.703125\n",
      "Train processing: step 99, loss 0.871531963348, accuracy 0.578125\n",
      "Train processing: step 100, loss 0.780882954597, accuracy 0.65625\n",
      "Evalution start... at step 100\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------Start training model...--------------------\")\n",
    "gra=tf.Graph()\n",
    "with gra.as_default():\n",
    "    sess=tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn=cnn.topicCNN(vocabulary_size=vocabulary_size,embedding_size=FLAGS.embedding_size,\n",
    "                         filter_size=map(int,FLAGS.filter_size.split(',')),\n",
    "                         filter_num=FLAGS.filter_num,max_news_size=max_news_length,topic_size=3)\n",
    "        \n",
    "        global_step=tf.Variable(0,name=\"global_step\",trainable=False)\n",
    "        optimizer=tf.train.AdamOptimizer()\n",
    "        gradient_and_variable=optimizer.compute_gradients(cnn.loss)\n",
    "        train_op=optimizer.apply_gradients(gradient_and_variable,global_step=global_step)\n",
    "        \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        def train_one_step(x_batch,y_batch):\n",
    "            feed_dict={cnn.input_news:x_batch,cnn.input_topic:y_batch,\n",
    "                       cnn.dropout_keep_probability:FLAGS.keep_prob}\n",
    "            _,step,loss,accuracy=sess.run([train_op,global_step,cnn.loss,cnn.accuracy],feed_dict)\n",
    "            print(\"Train processing: step {}, loss {}, accuracy {}\".format(step,loss,accuracy))\n",
    "        \n",
    "        def dev_one_step(x_batch,y_batch):\n",
    "            feed_dict={cnn.input_news:x_batch,cnn.input_topic:y_batch,\n",
    "                       cnn.dropout_keep_probability:1.0}\n",
    "            step,loss,accuracy=sess.run([global_step,cnn.loss,cnn.accuracy],feed_dict)\n",
    "            print(\"Dev processing: step {}, loss {}, accuracy {}\".format(step,loss,accuracy))\n",
    "        \n",
    "        \n",
    "        for epo in range(FLAGS.epoch_size):\n",
    "            print('---------------Epoch: %s---------------' % epo)\n",
    "            # input data in each epoch is not be permutated!\n",
    "            for i in range(len(y_train)//FLAGS.batch_size):\n",
    "                x_temp=x_train[i*FLAGS.batch_size:(i+1)*FLAGS.batch_size]\n",
    "                y_temp=y_train[i*FLAGS.batch_size:(i+1)*FLAGS.batch_size]\n",
    "                train_one_step(x_temp,y_temp)\n",
    "                current_step=tf.train.global_step(sess,global_step)\n",
    "                if current_step % FLAGS.evaluate_step==0:\n",
    "                    print(\"Evalution start... at step %s\"%current_step)\n",
    "                    dev_one_step(x_dev,y_dev)\n",
    "                    print(\"Evaluation end\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
